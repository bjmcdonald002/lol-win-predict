{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c40119-7e67-435d-bf28-3cf1366bd8f7",
   "metadata": {},
   "source": [
    "# Binary Classification Model Comparison\n",
    "\n",
    "This notebook explores and compares multiple machine learning methods for a **binary classification problem** using ranked *League of Legends* solo/duo match data.\n",
    "\n",
    "The primary goals of this analysis are:\n",
    "- To **train and evaluate** a variety of classification models\n",
    "- To **identify top-performing models** based on validation performance\n",
    "- To **verify robustness** using both **shuffle testing** and **k-fold cross-validation**\n",
    "\n",
    "The notebook includes:\n",
    "- Data preprocessing and feature selection\n",
    "- Implementation of various classification models (e.g., logistic regression, ensemble methods, neural networks)\n",
    "- Model evaluation and comparison\n",
    "- Post-model validation using shuffle tests and k-fold cross-validation\n",
    "\n",
    "The final result is a selection of the best-performing models, supported by rigorous validation to assess generalization performance.\n",
    "\n",
    "*Prepared by Barrett James McDonald | PhD Student | University of South Florida*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d203bd4-a89b-4603-8b5e-67ee19b4677b",
   "metadata": {},
   "source": [
    "> **Reproducibility Note:**  \n",
    "> All stochastic methods in this notebook (data splits, shuffles, and models) use `random_state=12` for reproducibility.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cdb34b-73c7-475f-a77d-4c0aef73b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "\n",
    "#data preprocessing & splitting\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#gradient boosting models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38db6e-307e-43ec-b9bd-e8d565ab287b",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing and Subsampling\n",
    "\n",
    "This section loads the raw match data, removes irrelevant metadata, filters for ranked solo/duo *CLASSIC* games, and prepares the cleaned numerical dataset for modeling. A random subsample of 2,500 observations is used to enable fast experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dd224b-abf4-4300-b2be-61c15b3ac4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_csv = pd.read_csv(\"league_data.csv\", dtype={'win': str})\n",
    "\n",
    "# Drop irrelevant/metadata columns\n",
    "columns_to_drop = [\n",
    "    'game_id', 'game_version', 'participant_id', 'puuid', 'summoner_name', 'summoner_id',\n",
    "    'solo_tier', 'solo_rank', 'solo_lp', 'solo_wins', 'solo_losses',\n",
    "    'flex_tier', 'flex_rank', 'flex_lp', 'flex_wins', 'flex_losses',\n",
    "    'champion_mastery_lastPlayTime', 'champion_mastery_lastPlayTime_utc',\n",
    "    'champion_id', 'map_id', 'platform_id', 'game_type', 'team_id',\n",
    "    'game_start_utc', 'queue_id', 'game_mode'\n",
    "]\n",
    "\n",
    "# Filter for CLASSIC + ranked solo/duo games\n",
    "df_filtered = df_csv[(df_csv['game_mode'] == 'CLASSIC') & (df_csv['queue_id'] == 420)].copy()\n",
    "\n",
    "# Drop metadata columns\n",
    "df_filtered_cleaned = df_filtered.drop(columns=[col for col in columns_to_drop if col in df_filtered.columns])\n",
    "\n",
    "# Convert 'win' column to binary\n",
    "df_filtered_cleaned['win'] = (df_filtered_cleaned['win'] == 'TRUE').astype(int)\n",
    "\n",
    "# Drop non-numeric/categorical columns (and item columns)\n",
    "df_numeric_only = df_filtered_cleaned.drop(columns=df_filtered_cleaned.select_dtypes(include=['object', 'category']).columns)\n",
    "df_numeric_only = df_numeric_only.drop(columns=[col for col in df_numeric_only.columns if col.startswith(\"item\")])\n",
    "\n",
    "# Final predictor/response matrices\n",
    "X = df_numeric_only.drop(columns=['win']).fillna(df_numeric_only.mean())\n",
    "y = df_numeric_only['win']\n",
    "\n",
    "# --- Subsample preparation (2,500 observations) ---\n",
    "#for reproducibility, set the numpy random seed to 12\n",
    "np.random.seed(12)\n",
    "sample_indices = np.random.choice(X.index, size=2500, replace=False)\n",
    "X_sample = X.loc[sample_indices]\n",
    "y_sample = y.loc[sample_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e6bb0-0483-45e5-ac91-3d14767e8618",
   "metadata": {},
   "source": [
    "## 2. Robust PCA + Logistic Regression\n",
    "\n",
    "This section uses a manually implemented **Robust PCA** to decompose the scaled predictor matrix into low-rank and sparse components. The low-rank matrix is used for further dimensionality reduction via PCA before fitting a logistic regression classifier.\n",
    "\n",
    "Performance metrics (Accuracy, F1 Score, ROC AUC) are computed on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e9c5eed-f8dd-4178-b135-282702a2cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCs for at least 80% variance: 16\n",
      "PCLR (Robust PCA + Logistic Regression) with 16 PCs (>=80% variance) (2500 row subsample):\n",
      "Accuracy:  0.8173\n",
      "F1 Score:  0.8185\n",
      "ROC AUC:   0.9050\n"
     ]
    }
   ],
   "source": [
    "# --- Defining Robust PCA Manual Calculation of L and S ---\n",
    "def robust_pca_fast(M, max_iter=150, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Perform Robust Principal Component Analysis (RPCA) on matrix M.\n",
    "    Decomposes M into L (low-rank) and S (sparse) components using\n",
    "    the Principal Component Pursuit algorithm.\n",
    "\n",
    "    Args:\n",
    "        M (np.ndarray): Input data matrix (rows = observations, cols = features)\n",
    "        max_iter (int): Maximum number of iterations\n",
    "        tol (float): Convergence tolerance (Frobenius norm of residual)\n",
    "\n",
    "    Returns:\n",
    "        L (np.ndarray): Low-rank matrix\n",
    "        S (np.ndarray): Sparse matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Helper function: soft-thresholding operator ---\n",
    "    def shrinkage_operator(x, tau):\n",
    "        # Applies soft-thresholding elementwise\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - tau, 0.)\n",
    "\n",
    "    # --- Helper function: thresholded SVD ---\n",
    "    def svd_thresholding_operator(X, tau):\n",
    "        # Applies singular value thresholding to keep only large singular values\n",
    "        U, S, Vh = LA.svd(X, full_matrices=False)\n",
    "        S_thresh = shrinkage_operator(S, tau)\n",
    "        return U @ np.diag(S_thresh) @ Vh\n",
    "\n",
    "    # --- Initialization ---\n",
    "    S = np.zeros_like(M)              # Start with zero sparse matrix\n",
    "    Y = np.zeros_like(M)              # Lagrange multiplier (dual variable)\n",
    "    mu = np.prod(M.shape) / (4.0 * LA.norm(M, ord=1))  # Step size parameter\n",
    "    mu_inv = 1.0 / mu\n",
    "    lam = 1.0 / np.sqrt(np.max(M.shape))               # Regularization parameter\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # --- Low-rank update via SVD thresholding ---\n",
    "        L = svd_thresholding_operator(M - S + mu_inv * Y, mu_inv)\n",
    "\n",
    "        # --- Sparse matrix update via elementwise shrinkage ---\n",
    "        S = shrinkage_operator(M - L + mu_inv * Y, lam * mu_inv)\n",
    "\n",
    "        # --- Dual variable update (Lagrange multiplier) ---\n",
    "        Y = Y + mu * (M - L - S)\n",
    "\n",
    "        # --- Check convergence ---\n",
    "        error = LA.norm(M - L - S, ord='fro')\n",
    "        if error < tol:\n",
    "            break\n",
    "\n",
    "    return L, S\n",
    "    \n",
    "# --- Preprocessing and robust PCA ---\n",
    "scaler_raw = StandardScaler()\n",
    "X_scaled_sample = scaler_raw.fit_transform(X_sample)\n",
    "L_sample, S_sample = robust_pca_fast(X_scaled_sample)\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    L_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- PCA: find n_components for >=80% variance ---\n",
    "pca_full = PCA(random_state=12)\n",
    "pca_full.fit(X_train_scaled)\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_80 = np.searchsorted(cumulative_variance, 0.80) + 1  # +1 because index starts at 0\n",
    "\n",
    "print(f\"Number of PCs for at least 80% variance: {n_components_80}\")\n",
    "\n",
    "# --- Fit PCA with this n_components ---\n",
    "pca = PCA(n_components=n_components_80, random_state=12)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# --- Logistic Regression ---\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=12)\n",
    "log_reg.fit(X_train_pca, y_train)\n",
    "y_pred = log_reg.predict(X_test_pca)\n",
    "y_proba = log_reg.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "# --- Performance output ---\n",
    "print(f\"PCLR (Robust PCA + Logistic Regression) with {n_components_80} PCs (>=80% variance) (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5fdac-77ac-470c-8de0-2cc514ee1c99",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression (No PCA): L1 vs L2 Regularization\n",
    "\n",
    "This section evaluates logistic regression models trained directly on the full set of numeric features (no dimensionality reduction).\n",
    "\n",
    "Two forms of regularization are compared:\n",
    "- **L2 (Ridge):** Penalizes the squared magnitude of coefficients. Tends to shrink coefficients uniformly but keeps all features.\n",
    "- **L1 (Lasso):** Penalizes the absolute value of coefficients. Can set some coefficients exactly to zero, thus performing feature selection.\n",
    "\n",
    "We apply both penalties to the same training/test split of a 2,500-observation subsample and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bce67363-9e47-4b29-93c6-3d6e87e4cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression without PCA:\n",
      "L2 Regularization (2500 row subsample):\n",
      "Accuracy:  0.8853\n",
      "F1 Score:  0.8871\n",
      "ROC AUC:   0.9458\n",
      "\n",
      "L1 Regularization (2500 row subsample):\n",
      "Accuracy:  0.8853\n",
      "F1 Score:  0.8868\n",
      "ROC AUC:   0.9463\n"
     ]
    }
   ],
   "source": [
    "# --- Split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- L2 (Ridge) Regularization ---\n",
    "log_reg_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=12)\n",
    "log_reg_l2.fit(X_train_scaled, y_train)\n",
    "y_pred_l2 = log_reg_l2.predict(X_test_scaled)\n",
    "y_proba_l2 = log_reg_l2.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- L1 (Lasso) Regularization ---\n",
    "log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=12)\n",
    "log_reg_l1.fit(X_train_scaled, y_train)\n",
    "y_pred_l1 = log_reg_l1.predict(X_test_scaled)\n",
    "y_proba_l1 = log_reg_l1.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"Logistic Regression without PCA:\")\n",
    "\n",
    "print(\"L2 Regularization (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_l2):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_l2):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_l2):.4f}\\n\")\n",
    "\n",
    "print(\"L1 Regularization (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_l1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afab321-ca56-45e5-94a0-101cca4338d9",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Classifier\n",
    "\n",
    "This section implements a basic **Decision Tree Classifier** trained on the same scaled numeric data used in previous models.\n",
    "\n",
    "Decision Trees are intuitive and interpretable models that recursively split the feature space to classify observations. While they tend to overfit without pruning or regularization, they offer a useful baseline for tree-based methods like Random Forest and Gradient Boosted Trees.\n",
    "\n",
    "We evaluate the model on a 70/30 train-test split and report standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e04bf8a-4878-4c5a-9929-0c90bc1fbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results (2500 row subsample):\n",
      "Accuracy:  0.8000\n",
      "F1 Score:  0.8125\n",
      "ROC AUC:   0.7988\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Decision Tree ---\n",
    "tree_clf = DecisionTreeClassifier(random_state=12)\n",
    "tree_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test_scaled)\n",
    "y_proba_tree = tree_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Decision Tree Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_tree):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_tree):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_tree):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c0951-cded-48e2-b2dd-c08f562f158d",
   "metadata": {},
   "source": [
    "## 5. Random Forest Classifier\n",
    "\n",
    "This section applies a **Random Forest**, an ensemble learning method that builds a collection of decision trees and combines their predictions to improve generalization.\n",
    "\n",
    "Random Forests reduce overfitting by:\n",
    "- Training each tree on a random bootstrap sample of the data\n",
    "- Using a random subset of features at each split\n",
    "\n",
    "Here, we train a forest of 100 trees on a 70/30 split of the scaled data and report accuracy, F1 score, and ROC AUC. This provides a more robust baseline than a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "458cc7a5-99ce-4717-a984-e52a7f8d67d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results (2500 row subsample):\n",
      "Accuracy:  0.8573\n",
      "F1 Score:  0.8633\n",
      "ROC AUC:   0.9340\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Random Forest ---\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=12)\n",
    "forest_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_forest = forest_clf.predict(X_test_scaled)\n",
    "y_proba_forest = forest_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Random Forest Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_forest):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_forest):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_forest):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d73cfb-8e15-415d-88f3-91c911b06f95",
   "metadata": {},
   "source": [
    "## 6. XGBoost Classifier\n",
    "\n",
    "This section trains an **Extreme Gradient Boosting (XGBoost)** classifier, a high-performance ensemble method known for its ability to handle:\n",
    "- Imbalanced classes\n",
    "- Nonlinear feature interactions\n",
    "- Feature importance and missing data\n",
    "\n",
    "XGBoost builds trees sequentially, where each new tree tries to correct the errors of the previous one, minimizing a specified loss function—in this case, **log loss**.\n",
    "\n",
    "We train the model on scaled data with default hyperparameters and evaluate using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f634bafe-3f1b-4b1b-8ee6-4ad0d3aaeac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results (2500 row subsample):\n",
      "Accuracy:  0.8693\n",
      "F1 Score:  0.8753\n",
      "ROC AUC:   0.9407\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train XGBoost ---\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss', random_state=12)\n",
    "xgb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"XGBoost Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbede2a2-a374-46f2-87ba-b4aadf21a2ca",
   "metadata": {},
   "source": [
    "## 7. LightGBM Classifier\n",
    "\n",
    "This section fits a **LightGBM (Light Gradient Boosting Machine)** model—an efficient gradient boosting framework that uses histogram-based algorithms for faster training and lower memory usage.\n",
    "\n",
    "Compared to XGBoost, LightGBM is:\n",
    "- Faster on large datasets with many features\n",
    "- Capable of handling categorical variables natively (though we use numeric-only data here)\n",
    "- Often just as accurate (or better) with less tuning\n",
    "\n",
    "We train LightGBM on a 70/30 train-test split of scaled data, evaluating its predictive performance with standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b53cd65d-ec1f-4297-a987-201dd18c2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Results (2500 row subsample):\n",
      "Accuracy:  0.8667\n",
      "F1 Score:  0.8724\n",
      "ROC AUC:   0.9416\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# --- Train LightGBM ---\n",
    "lgbm_clf = LGBMClassifier(verbose=-1, random_state=12)\n",
    "lgbm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_test_scaled)\n",
    "y_proba_lgbm = lgbm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"LightGBM Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_lgbm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8752382-f6c4-453b-abb7-e2c73ae3a26e",
   "metadata": {},
   "source": [
    "## 8. Support Vector Machine (RBF Kernel)\n",
    "\n",
    "This section trains a **Support Vector Machine (SVM)** with a radial basis function (RBF) kernel. SVMs aim to find the optimal hyperplane that separates classes with the **maximum margin** in a high-dimensional space.\n",
    "\n",
    "Key notes:\n",
    "- The **RBF kernel** allows the model to learn nonlinear decision boundaries by implicitly mapping features into a higher-dimensional space.\n",
    "- We set `probability=True` to enable **probability estimates**, which are required for computing the **ROC AUC** score.\n",
    "\n",
    "Although SVMs can be computationally intensive, especially on large datasets, they often perform well with clean, scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b772a541-26f2-473e-b83a-59c75e770d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Results (2500 row subsample):\n",
      "Accuracy:  0.8667\n",
      "F1 Score:  0.8731\n",
      "ROC AUC:   0.9338\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train SVM (with probability enabled for ROC AUC) ---\n",
    "svm_clf = SVC(probability=True, kernel='rbf', random_state=12)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test_scaled)\n",
    "y_proba_svm = svm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Support Vector Machine Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1083e4cb-dd41-4468-a3b2-af129f2c517c",
   "metadata": {},
   "source": [
    "## 9. K-Nearest Neighbors (KNN)\n",
    "\n",
    "This section applies the **K-Nearest Neighbors (KNN)** algorithm using \\( k = 5 \\). KNN is a **non-parametric** method that classifies a sample based on the majority label among its nearest neighbors in feature space.\n",
    "\n",
    "Key characteristics:\n",
    "- Simple, interpretable, and effective with well-scaled, low-dimensional data\n",
    "- Sensitive to irrelevant features and class imbalance\n",
    "- Performance depends heavily on the choice of **k** and the distance metric\n",
    "\n",
    "Here, we standardize features and evaluate KNN on a 70/30 split of a 2,500-observation subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ce1a411-2d84-433d-9587-996780bbec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Results (2500 row subsample):\n",
      "Accuracy:  0.7653\n",
      "F1 Score:  0.7596\n",
      "ROC AUC:   0.8444\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train KNN ---\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "y_proba_knn = knn_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"K-Nearest Neighbors Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_knn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed9c0c-4edb-4c98-a314-a2366b588920",
   "metadata": {},
   "source": [
    "## 10. Naive Bayes Classifier\n",
    "\n",
    "This section implements a **Naive Bayes classifier**, specifically using the **GaussianNB** variant. Naive Bayes is a probabilistic model based on Bayes’ Theorem, assuming **feature independence** given the class label.\n",
    "\n",
    "Why it matters:\n",
    "- Surprisingly effective in high-dimensional settings\n",
    "- Fast to train and easy to interpret\n",
    "- Works best when features are roughly independent (which is rare, but doesn’t always hurt performance)\n",
    "\n",
    "We fit the model on scaled data and evaluate using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2213e8db-5077-43d4-9025-c916c98609e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results (2500 row subsample):\n",
      "Accuracy:  0.7280\n",
      "F1 Score:  0.7018\n",
      "ROC AUC:   0.8289\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Naive Bayes ---\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_test_scaled)\n",
    "y_proba_nb = nb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Naive Bayes Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_nb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556a17a-8fc4-41ab-aeff-ccf943e6c79d",
   "metadata": {},
   "source": [
    "## 11. Neural Network: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "This final model is a **Neural Network**, specifically a **Multi-Layer Perceptron (MLP)**. MLPs are **feedforward neural networks** that learn complex, nonlinear relationships through layers of interconnected neurons.\n",
    "\n",
    "Model architecture:\n",
    "- One hidden layer with 100 neurons\n",
    "- Uses ReLU activation (default)\n",
    "- Optimized with the Adam solver\n",
    "- Trained for up to 500 iterations (or until convergence)\n",
    "\n",
    "While MLPs often require more tuning and training time, they can outperform traditional models when the data contains deep, abstract patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9727e336-d259-4be2-9834-7f462f910cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network (MLP) Results (2500 row subsample):\n",
      "Accuracy:  0.8573\n",
      "F1 Score:  0.8630\n",
      "ROC AUC:   0.9376\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.3, stratify=y_sample, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Neural Network (MLP) ---\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=12)\n",
    "mlp_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_mlp = mlp_clf.predict(X_test_scaled)\n",
    "y_proba_mlp = mlp_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Neural Network (MLP) Results (2500 row subsample):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_mlp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44740a1b-9fe6-4aaf-93d8-055e31c59dc8",
   "metadata": {},
   "source": [
    "## Note on Model Selection:\n",
    "After evaluating all candidate models on subsampled data, we identified the five models with the strongest validation performance (based on ROC AUC, F1 score, and accuracy).  \n",
    "\n",
    "To provide a robust assessment of their true predictive capabilities, we now retrain and evaluate these top five models on the entire, cleaned dataset.  \n",
    " \n",
    "This ensures that our final comparisons reflect each model's ability to generalize across the full feature space and observation set, and forms the basis for the results reported in the main tables of this study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1ac1b-03a4-444a-bb94-112e56199ea5",
   "metadata": {},
   "source": [
    "## 12. Tuned Neural Network (MLP) on Full Dataset\n",
    "\n",
    "In this model, we revisit the **Multi-Layer Perceptron (MLP)** and apply **hyperparameter tuning** for improved performance. Key changes include:\n",
    "\n",
    "- **Architecture:** Two hidden layers with 128 and 64 neurons\n",
    "- **Activation:** ReLU (Rectified Linear Unit)\n",
    "- **Optimizer:** Adam (adaptive learning rate)\n",
    "- **Training:** 1,000 epochs (or until convergence)\n",
    "- **Dataset:** Full cleaned dataset (not a subsample)\n",
    "\n",
    "These changes aim to increase the model's capacity to learn complex, nonlinear relationships across the full feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8beb917c-81ac-464d-9741-2528c9d873c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Neural Network Results (Full Dataset):\n",
      "Accuracy:  0.8781\n",
      "F1 Score:  0.8746\n",
      "ROC AUC:   0.9475\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, stratify=y_full, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Tuned MLP Neural Network ---\n",
    "mlp_tuned = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=12\n",
    ")\n",
    "mlp_tuned.fit(X_train_scaled, y_train)\n",
    "y_pred_mlp = mlp_tuned.predict(X_test_scaled)\n",
    "y_proba_mlp = mlp_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Tuned Neural Network Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_mlp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd283a-a7af-4572-93cd-ea82634ed3de",
   "metadata": {},
   "source": [
    "## 13. LightGBM on Full Dataset\n",
    "\n",
    "This model revisits **LightGBM**, this time training on the entire preprocessed dataset rather than a subsample.\n",
    "\n",
    "By scaling and fitting LightGBM on the full data, we aim to:\n",
    "- Leverage more information for training\n",
    "- Capture rarer patterns that may not be present in smaller subsamples\n",
    "- Potentially improve performance and stability\n",
    "\n",
    "We retain feature names by converting the scaled arrays back into DataFrames, which can help with future interpretability and feature importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0056dde2-db4a-439d-838a-9518524f6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Results (Full Dataset):\n",
      "Accuracy:  0.8905\n",
      "F1 Score:  0.8902\n",
      "ROC AUC:   0.9636\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, stratify=y_full, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# --- Train LightGBM ---\n",
    "lgbm_clf = LGBMClassifier(verbose=-1, random_state=12)\n",
    "lgbm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_test_scaled)\n",
    "y_proba_lgbm = lgbm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"LightGBM Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_lgbm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e425f-cd42-4ecf-9f66-8b5d5990e40b",
   "metadata": {},
   "source": [
    "## 14. XGBoost on Full Dataset\n",
    "\n",
    "This section returns to **XGBoost**, but this time we train on the entire preprocessed dataset instead of a subsample.\n",
    "\n",
    "Key details:\n",
    "- Full dataset provides a richer training signal, potentially capturing subtler relationships\n",
    "- XGBoost is configured to minimize **log loss**, which is appropriate for binary classification with probabilistic outputs\n",
    "- Data is standardized before training, though XGBoost can handle unscaled input—scaling helps ensure fair comparison across models\n",
    "\n",
    "We evaluate model performance using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ee57ff7-f87e-4d2d-aa68-b123abd2bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results (Full Dataset):\n",
      "Accuracy:  0.8921\n",
      "F1 Score:  0.8916\n",
      "ROC AUC:   0.9625\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, stratify=y_full, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train XGBoost ---\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss', random_state=12)\n",
    "xgb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"XGBoost Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0725d4-84f8-4580-9d15-39f8c8ff24b1",
   "metadata": {},
   "source": [
    "## 15. L1-Regularized Logistic Regression on Full Dataset\n",
    "\n",
    "Here we train a **logistic regression model with L1 (Lasso) regularization** on the full dataset.\n",
    "\n",
    "L1 regularization:\n",
    "- Encourages sparsity in the model coefficients (i.e., some become exactly zero)\n",
    "- Effectively performs **feature selection** by shrinking less informative features\n",
    "- Helps prevent overfitting when many features are present\n",
    "\n",
    "We scale the full dataset before training, and evaluate using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95ff6f26-5ecf-48b3-b272-38a925117434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression Results (Full Dataset):\n",
      "Accuracy:  0.8822\n",
      "F1 Score:  0.8806\n",
      "ROC AUC:   0.9464\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, stratify=y_full, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- L1 (Lasso) Logistic Regression ---\n",
    "log_reg_l1 = LogisticRegression(\n",
    "    penalty='l1', solver='liblinear', max_iter=1000, random_state=12\n",
    ")\n",
    "log_reg_l1.fit(X_train_scaled, y_train)\n",
    "y_pred_l1 = log_reg_l1.predict(X_test_scaled)\n",
    "y_proba_l1 = log_reg_l1.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"L1 Logistic Regression Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_l1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfbcf03-0ecb-400c-a73c-7e61089efaa5",
   "metadata": {},
   "source": [
    "## 16. L2-Regularized Logistic Regression on Full Dataset\n",
    "\n",
    "Here we train a **logistic regression model with L2 (Ridge) regularization** on the full dataset.\n",
    "\n",
    "L2 regularization:\n",
    "- Penalizes large coefficients, but does **not** enforce sparsity (all features usually retained)\n",
    "- Tends to distribute weights more evenly among correlated features\n",
    "- Helps prevent overfitting, especially when features are collinear or numerous\n",
    "\n",
    "We scale the full dataset before training, and evaluate using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "110516a2-1cd2-4e09-a424-ba57bd5d3492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Logistic Regression Results (Full Dataset):\n",
      "Accuracy:  0.8812\n",
      "F1 Score:  0.8796\n",
      "ROC AUC:   0.9462\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y_full, test_size=0.3, stratify=y_full, random_state=12\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- L2 (Ridge) Logistic Regression ---\n",
    "log_reg_l2 = LogisticRegression(\n",
    "    penalty='l2', solver='lbfgs', max_iter=1000, random_state=12\n",
    ")\n",
    "log_reg_l2.fit(X_train_scaled, y_train)\n",
    "y_pred_l2 = log_reg_l2.predict(X_test_scaled)\n",
    "y_proba_l2 = log_reg_l2.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"L2 Logistic Regression Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_l2):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_l2):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_l2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc7e6c-9531-4d15-ab2e-cb7ac54bb63c",
   "metadata": {},
   "source": [
    "## Model Validation Strategy:\n",
    "\n",
    "To ensure the reliability and generalizability of our findings, we perform **two complementary validation methods** for each top-performing model:  \n",
    " \n",
    "**Shuffle tests:** For each model, we repeat the shuffle test multiple times. This involves randomly permuting the outcome labels and retraining the model, confirming that predictive performance drops to chance levels (mean ROC AUC, F1, and accuracy all ≈ 0.5). This rules out model bias, data leakage, or spurious relationships.\n",
    " \n",
    "**K-fold cross-validation:** We use 5-fold stratified cross-validation to assess each model’s stability and predictive performance across different train/test splits. We report the mean and standard deviation of ROC AUC, along with average F1 score and accuracy across folds.\n",
    " \n",
    "These validation steps provide strong evidence that our results are robust and not an artifact of data sampling or model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e60f0f-7a08-4d84-a6d1-2438fe174ae9",
   "metadata": {},
   "source": [
    "## 16. Shuffle Test Results for LightGBM & XGBoost\n",
    "\n",
    "The table below reports mean ROC AUC, F1 score, and accuracy for 10 shuffle runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0337368-4167-48ee-84ca-f519046ed6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM (Shuffled Labels, 10 runs):\n",
      "AUCs: [0.5026 0.5041 0.5009 0.4967 0.5019 0.5046 0.4932 0.4964 0.5018 0.5105]\n",
      "F1s: [0.4986 0.4989 0.4962 0.4951 0.5078 0.5107 0.4982 0.4993 0.5009 0.5108]\n",
      "Accuracies: [0.5043 0.5037 0.5016 0.4964 0.5028 0.5056 0.4922 0.4981 0.497  0.5094]\n",
      "Mean AUC: 0.5013 | Std Dev: 0.0047\n",
      "Mean F1: 0.5017\n",
      "Mean Accuracy: 0.5011\n",
      "\n",
      "XGBoost (Shuffled Labels, 10 runs):\n",
      "AUCs: [0.5029 0.4992 0.4978 0.5009 0.503  0.502  0.4926 0.4965 0.5066 0.5088]\n",
      "F1s: [0.5078 0.4967 0.4969 0.5011 0.5058 0.5042 0.4932 0.5    0.5051 0.515 ]\n",
      "Accuracies: [0.508  0.4943 0.4978 0.5025 0.5008 0.4998 0.4941 0.4995 0.5045 0.51  ]\n",
      "Mean AUC: 0.5010 | Std Dev: 0.0045\n",
      "Mean F1: 0.5026\n",
      "Mean Accuracy: 0.5011\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "\n",
    "lgbm_auc_shuff, lgbm_f1_shuff, lgbm_acc_shuff = [], [], []\n",
    "xgb_auc_shuff, xgb_f1_shuff, xgb_acc_shuff = [], [], []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # --- Shuffle labels independently (different seed each time) ---\n",
    "    y_shuffled = y_full.sample(frac=1, random_state=12 + i).reset_index(drop=True)\n",
    "    X_shuffled = X_full.reset_index(drop=True)  # Align index with shuffled y\n",
    "\n",
    "    # --- Train/test split and scale (on mismatched X/y) ---\n",
    "    X_train, X_test, y_train_shuffled, y_test_shuffled = train_test_split(\n",
    "        X_shuffled, y_shuffled, test_size=0.3, stratify=y_shuffled, random_state=12 + i\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    # --- LightGBM on shuffled labels ---\n",
    "    lgbm_clf = LGBMClassifier(verbose=-1, random_state=12)\n",
    "    lgbm_clf.fit(X_train_scaled, y_train_shuffled)\n",
    "    y_pred_lgbm = lgbm_clf.predict(X_test_scaled)\n",
    "    y_proba_lgbm = lgbm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    lgbm_auc_shuff.append(roc_auc_score(y_test_shuffled, y_proba_lgbm))\n",
    "    lgbm_f1_shuff.append(f1_score(y_test_shuffled, y_pred_lgbm))\n",
    "    lgbm_acc_shuff.append(accuracy_score(y_test_shuffled, y_pred_lgbm))\n",
    "\n",
    "    # --- XGBoost on shuffled labels ---\n",
    "    xgb_clf = XGBClassifier(eval_metric='logloss', random_state=12)\n",
    "    xgb_clf.fit(X_train_scaled, y_train_shuffled)\n",
    "    y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
    "    y_proba_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    xgb_auc_shuff.append(roc_auc_score(y_test_shuffled, y_proba_xgb))\n",
    "    xgb_f1_shuff.append(f1_score(y_test_shuffled, y_pred_xgb))\n",
    "    xgb_acc_shuff.append(accuracy_score(y_test_shuffled, y_pred_xgb))\n",
    "\n",
    "# --- Results ---\n",
    "print(\"LightGBM (Shuffled Labels, 10 runs):\")\n",
    "print(\"AUCs:\", np.round(lgbm_auc_shuff, 4))\n",
    "print(\"F1s:\", np.round(lgbm_f1_shuff, 4))\n",
    "print(\"Accuracies:\", np.round(lgbm_acc_shuff, 4))\n",
    "print(f\"Mean AUC: {np.mean(lgbm_auc_shuff):.4f} | Std Dev: {np.std(lgbm_auc_shuff):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(lgbm_f1_shuff):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(lgbm_acc_shuff):.4f}\\n\")\n",
    "\n",
    "print(\"XGBoost (Shuffled Labels, 10 runs):\")\n",
    "print(\"AUCs:\", np.round(xgb_auc_shuff, 4))\n",
    "print(\"F1s:\", np.round(xgb_f1_shuff, 4))\n",
    "print(\"Accuracies:\", np.round(xgb_acc_shuff, 4))\n",
    "print(f\"Mean AUC: {np.mean(xgb_auc_shuff):.4f} | Std Dev: {np.std(xgb_auc_shuff):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(xgb_f1_shuff):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(xgb_acc_shuff):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320b2d1-a831-4593-84c3-20f9e7e32d20",
   "metadata": {},
   "source": [
    "## 17. K-Fold Cross-Validation for LightGBM & XGBoost\n",
    "\n",
    "The table below summarizes mean ROC AUC, F1 score, and accuracy across 5 folds for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ef87a051-6b98-4c97-b35a-d673c03af150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM K-Fold:\n",
      "ROC AUCs: [0.9668 0.968  0.9648 0.9634 0.9672]\n",
      "F1s: [0.8983 0.9028 0.894  0.8897 0.9049]\n",
      "Accuracies: [0.8984 0.9035 0.8953 0.89   0.9044]\n",
      "Mean AUC: 0.9661 | Std Dev: 0.0017\n",
      "Mean F1: 0.8979\n",
      "Mean Accuracy: 0.8983\n",
      "\n",
      "XGBoost K-Fold:\n",
      "ROC AUCs: [0.9651 0.9674 0.963  0.9627 0.9671]\n",
      "F1s: [0.9016 0.9013 0.8942 0.8917 0.9016]\n",
      "Accuracies: [0.9019 0.9016 0.895  0.8923 0.9016]\n",
      "Mean AUC: 0.9651 | Std Dev: 0.0020\n",
      "Mean F1: 0.8981\n",
      "Mean Accuracy: 0.8985\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Scale full data ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)  # Retain column names\n",
    "\n",
    "# --- 5-Fold Stratified CV ---\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "lgbm_aucs, lgbm_f1s, lgbm_accs = [], [], []\n",
    "xgb_aucs, xgb_f1s, xgb_accs = [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled_df, y_full):\n",
    "    X_train, X_test = X_scaled_df.iloc[train_index], X_scaled_df.iloc[test_index]\n",
    "    y_train, y_test = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm_model = LGBMClassifier(verbose=-1, random_state=12)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    lgbm_pred = lgbm_model.predict(X_test)\n",
    "    lgbm_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "    lgbm_auc = roc_auc_score(y_test, lgbm_proba)\n",
    "    lgbm_f1 = f1_score(y_test, lgbm_pred)\n",
    "    lgbm_acc = accuracy_score(y_test, lgbm_pred)\n",
    "    lgbm_aucs.append(lgbm_auc)\n",
    "    lgbm_f1s.append(lgbm_f1)\n",
    "    lgbm_accs.append(lgbm_acc)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = XGBClassifier(eval_metric='logloss', random_state=12)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test)\n",
    "    xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
    "    xgb_f1 = f1_score(y_test, xgb_pred)\n",
    "    xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "    xgb_aucs.append(xgb_auc)\n",
    "    xgb_f1s.append(xgb_f1)\n",
    "    xgb_accs.append(xgb_acc)\n",
    "\n",
    "# --- Results ---\n",
    "print(\"LightGBM K-Fold:\")\n",
    "print(\"ROC AUCs:\", np.round(lgbm_aucs, 4))\n",
    "print(\"F1s:\", np.round(lgbm_f1s, 4))\n",
    "print(\"Accuracies:\", np.round(lgbm_accs, 4))\n",
    "print(f\"Mean AUC: {np.mean(lgbm_aucs):.4f} | Std Dev: {np.std(lgbm_aucs):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(lgbm_f1s):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(lgbm_accs):.4f}\\n\")\n",
    "\n",
    "print(\"XGBoost K-Fold:\")\n",
    "print(\"ROC AUCs:\", np.round(xgb_aucs, 4))\n",
    "print(\"F1s:\", np.round(xgb_f1s, 4))\n",
    "print(\"Accuracies:\", np.round(xgb_accs, 4))\n",
    "print(f\"Mean AUC: {np.mean(xgb_aucs):.4f} | Std Dev: {np.std(xgb_aucs):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(xgb_f1s):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(xgb_accs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0630b3b-5ebb-4542-a902-c3fdde5212be",
   "metadata": {},
   "source": [
    "## 18. Shuffle Test Results for L1 & L2 Regularized Logistic Regression\n",
    "\n",
    "The table below reports mean ROC AUC, F1 score, and accuracy for 10 shuffle runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "97eab939-299f-4c65-86ec-221e7a18bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression (Shuffled Labels, 10 runs):\n",
      "AUCs: [0.5096 0.5002 0.4921 0.5044 0.4934 0.5024 0.5112 0.4865 0.504  0.4949]\n",
      "F1s: [0.4958 0.4864 0.4835 0.5087 0.4961 0.4915 0.5099 0.5019 0.506  0.4865]\n",
      "Accuracies: [0.5027 0.503  0.4938 0.5001 0.4954 0.4975 0.513  0.4911 0.5054 0.497 ]\n",
      "Mean AUC: 0.4999 | Std Dev: 0.0076\n",
      "Mean F1: 0.4966\n",
      "Mean Accuracy: 0.4999\n",
      "\n",
      "L2 Logistic Regression (Shuffled Labels, 10 runs):\n",
      "AUCs: [0.5096 0.5003 0.4927 0.5048 0.4935 0.5028 0.5119 0.4858 0.5039 0.4949]\n",
      "F1s: [0.4979 0.4831 0.4827 0.5104 0.4967 0.4951 0.5098 0.4984 0.502  0.4867]\n",
      "Accuracies: [0.5054 0.5008 0.4943 0.503  0.496  0.5001 0.5135 0.4885 0.5028 0.4961]\n",
      "Mean AUC: 0.5000 | Std Dev: 0.0078\n",
      "Mean F1: 0.4963\n",
      "Mean Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "\n",
    "l1_auc_shuff, l1_f1_shuff, l1_acc_shuff = [], [], []\n",
    "l2_auc_shuff, l2_f1_shuff, l2_acc_shuff = [], [], []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # --- Shuffle labels independently ---\n",
    "    _, y_shuffled = shuffle(X_scaled, y_full, random_state=12 + i)\n",
    "    \n",
    "    X_train, X_test, y_train_shuff, y_test_shuff = train_test_split(\n",
    "        X_scaled, y_shuffled, test_size=0.3, stratify=y_shuffled, random_state=12 + i\n",
    "    )\n",
    "    \n",
    "    # --- L1 Logistic Regression ---\n",
    "    logreg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=12)\n",
    "    logreg_l1.fit(X_train, y_train_shuff)\n",
    "    y_pred_l1 = logreg_l1.predict(X_test)\n",
    "    y_proba_l1 = logreg_l1.predict_proba(X_test)[:, 1]\n",
    "    l1_auc_shuff.append(roc_auc_score(y_test_shuff, y_proba_l1))\n",
    "    l1_f1_shuff.append(f1_score(y_test_shuff, y_pred_l1))\n",
    "    l1_acc_shuff.append(accuracy_score(y_test_shuff, y_pred_l1))\n",
    "    \n",
    "    # --- L2 Logistic Regression ---\n",
    "    logreg_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=12)\n",
    "    logreg_l2.fit(X_train, y_train_shuff)\n",
    "    y_pred_l2 = logreg_l2.predict(X_test)\n",
    "    y_proba_l2 = logreg_l2.predict_proba(X_test)[:, 1]\n",
    "    l2_auc_shuff.append(roc_auc_score(y_test_shuff, y_proba_l2))\n",
    "    l2_f1_shuff.append(f1_score(y_test_shuff, y_pred_l2))\n",
    "    l2_acc_shuff.append(accuracy_score(y_test_shuff, y_pred_l2))\n",
    "\n",
    "# --- Results ---\n",
    "import numpy as np\n",
    "\n",
    "print(\"L1 Logistic Regression (Shuffled Labels, 10 runs):\")\n",
    "print(\"AUCs:\", np.round(l1_auc_shuff, 4))\n",
    "print(\"F1s:\", np.round(l1_f1_shuff, 4))\n",
    "print(\"Accuracies:\", np.round(l1_acc_shuff, 4))\n",
    "print(f\"Mean AUC: {np.mean(l1_auc_shuff):.4f} | Std Dev: {np.std(l1_auc_shuff):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(l1_f1_shuff):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(l1_acc_shuff):.4f}\\n\")\n",
    "\n",
    "print(\"L2 Logistic Regression (Shuffled Labels, 10 runs):\")\n",
    "print(\"AUCs:\", np.round(l2_auc_shuff, 4))\n",
    "print(\"F1s:\", np.round(l2_f1_shuff, 4))\n",
    "print(\"Accuracies:\", np.round(l2_acc_shuff, 4))\n",
    "print(f\"Mean AUC: {np.mean(l2_auc_shuff):.4f} | Std Dev: {np.std(l2_auc_shuff):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(l2_f1_shuff):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(l2_acc_shuff):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773d723-0abb-4c6e-a083-e95e85754b94",
   "metadata": {},
   "source": [
    "## 19. K-Fold Cross-Validation Results for L1 & L2 Regularized Logistic Regression\n",
    "\n",
    "The table below summarizes mean ROC AUC, F1 score, and accuracy across 5 folds for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "791034ef-3659-4128-853f-0f1ffefb688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression (5-Fold CV):\n",
      "ROC AUCs: [0.9461 0.9516 0.9496 0.9433 0.9475]\n",
      "F1s: [0.8838 0.8865 0.8784 0.8754 0.887 ]\n",
      "Accuracies: [0.8857 0.8882 0.8806 0.8781 0.8884]\n",
      "Mean AUC: 0.9476 | Std Dev: 0.0029\n",
      "Mean F1: 0.8822\n",
      "Mean Accuracy: 0.8842\n",
      "\n",
      "L2 Logistic Regression (5-Fold CV):\n",
      "ROC AUCs: [0.946  0.9516 0.9495 0.9432 0.9474]\n",
      "F1s: [0.8842 0.8855 0.8796 0.8753 0.887 ]\n",
      "Accuracies: [0.8859 0.8873 0.8818 0.8779 0.8884]\n",
      "Mean AUC: 0.9475 | Std Dev: 0.0029\n",
      "Mean F1: 0.8823\n",
      "Mean Accuracy: 0.8843\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Scale full data ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "# --- 5-Fold Stratified CV ---\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "l1_aucs, l1_f1s, l1_accs = [], [], []\n",
    "l2_aucs, l2_f1s, l2_accs = [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled, y_full):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "\n",
    "    # --- L1 Logistic Regression ---\n",
    "    logreg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000, random_state=12)\n",
    "    logreg_l1.fit(X_train, y_train)\n",
    "    y_pred_l1 = logreg_l1.predict(X_test)\n",
    "    y_proba_l1 = logreg_l1.predict_proba(X_test)[:, 1]\n",
    "    l1_aucs.append(roc_auc_score(y_test, y_proba_l1))\n",
    "    l1_f1s.append(f1_score(y_test, y_pred_l1))\n",
    "    l1_accs.append(accuracy_score(y_test, y_pred_l1))\n",
    "\n",
    "    # --- L2 Logistic Regression ---\n",
    "    logreg_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, random_state=12)\n",
    "    logreg_l2.fit(X_train, y_train)\n",
    "    y_pred_l2 = logreg_l2.predict(X_test)\n",
    "    y_proba_l2 = logreg_l2.predict_proba(X_test)[:, 1]\n",
    "    l2_aucs.append(roc_auc_score(y_test, y_proba_l2))\n",
    "    l2_f1s.append(f1_score(y_test, y_pred_l2))\n",
    "    l2_accs.append(accuracy_score(y_test, y_pred_l2))\n",
    "\n",
    "# --- Results ---\n",
    "print(\"L1 Logistic Regression (5-Fold CV):\")\n",
    "print(\"ROC AUCs:\", np.round(l1_aucs, 4))\n",
    "print(\"F1s:\", np.round(l1_f1s, 4))\n",
    "print(\"Accuracies:\", np.round(l1_accs, 4))\n",
    "print(f\"Mean AUC: {np.mean(l1_aucs):.4f} | Std Dev: {np.std(l1_aucs):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(l1_f1s):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(l1_accs):.4f}\\n\")\n",
    "\n",
    "print(\"L2 Logistic Regression (5-Fold CV):\")\n",
    "print(\"ROC AUCs:\", np.round(l2_aucs, 4))\n",
    "print(\"F1s:\", np.round(l2_f1s, 4))\n",
    "print(\"Accuracies:\", np.round(l2_accs, 4))\n",
    "print(f\"Mean AUC: {np.mean(l2_aucs):.4f} | Std Dev: {np.std(l2_aucs):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(l2_f1s):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(l2_accs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad2da6-3aca-4761-be86-2a6cec085496",
   "metadata": {},
   "source": [
    "## 20. Shuffle Test Results for Tuned Neural Network\n",
    "\n",
    "The table below reports mean ROC AUC, F1 score, and accuracy for 10 shuffle runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "00f0bc16-98b7-4a91-bffb-a8c0ba6cfaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (Shuffled Labels, 10 runs):\n",
      "AUCs: [0.5035 0.4916 0.4886 0.4942 0.5035 0.509  0.4929 0.4903 0.5012 0.4942]\n",
      "F1s: [0.5091 0.4992 0.4866 0.502  0.5031 0.5041 0.4908 0.5082 0.5084 0.4892]\n",
      "Accuracies: [0.5021 0.4983 0.4908 0.4914 0.5068 0.5081 0.4978 0.496  0.4998 0.4929]\n",
      "Mean AUC: 0.4969 | Std Dev: 0.0065\n",
      "Mean F1: 0.5001\n",
      "Mean Accuracy: 0.4984\n"
     ]
    }
   ],
   "source": [
    "n_runs = 10\n",
    "\n",
    "mlp_auc_shuff, mlp_f1_shuff, mlp_acc_shuff = [], [], []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    _, y_shuffled = shuffle(X_scaled, y_full, random_state=12 + i)\n",
    "\n",
    "    X_train, X_test, y_train_shuff, y_test_shuff = train_test_split(\n",
    "        X_scaled, y_shuffled, test_size=0.3, stratify=y_shuffled, random_state=12 + i\n",
    "    )\n",
    "\n",
    "    mlp_clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        random_state=12\n",
    "    )\n",
    "    mlp_clf.fit(X_train, y_train_shuff)\n",
    "    y_pred_mlp = mlp_clf.predict(X_test)\n",
    "    y_proba_mlp = mlp_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    mlp_auc_shuff.append(roc_auc_score(y_test_shuff, y_proba_mlp))\n",
    "    mlp_f1_shuff.append(f1_score(y_test_shuff, y_pred_mlp))\n",
    "    mlp_acc_shuff.append(accuracy_score(y_test_shuff, y_pred_mlp))\n",
    "\n",
    "# --- Results ---\n",
    "print(\"MLP (Shuffled Labels, 10 runs):\")\n",
    "print(\"AUCs:\", np.round(mlp_auc_shuff, 4))\n",
    "print(\"F1s:\", np.round(mlp_f1_shuff, 4))\n",
    "print(\"Accuracies:\", np.round(mlp_acc_shuff, 4))\n",
    "print(f\"Mean AUC: {np.mean(mlp_auc_shuff):.4f} | Std Dev: {np.std(mlp_auc_shuff):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(mlp_f1_shuff):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(mlp_acc_shuff):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efd8d7-bac0-41e0-8841-9f4987584a57",
   "metadata": {},
   "source": [
    "## 21. K-Fold Cross-Validation Results for Tuned Neural Network\n",
    "\n",
    "The table below summarizes mean ROC AUC, F1 score, and accuracy across 5 folds for the tuned MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8ccae9b6-8d4d-48aa-b428-b565a077cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (5-Fold CV):\n",
      "ROC AUCs: [0.9442 0.9511 0.9462 0.944  0.9488]\n",
      "F1s: [0.872  0.8863 0.8785 0.8727 0.8804]\n",
      "Accuracies: [0.874  0.887  0.8788 0.8736 0.8795]\n",
      "Mean AUC: 0.9469 | Std Dev: 0.0028\n",
      "Mean F1: 0.8780\n",
      "Mean Accuracy: 0.8786\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "mlp_aucs, mlp_f1s, mlp_accs = [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled, y_full):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "\n",
    "    mlp_clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=1000,\n",
    "        random_state=12\n",
    "    )\n",
    "    mlp_clf.fit(X_train, y_train)\n",
    "    y_pred_mlp = mlp_clf.predict(X_test)\n",
    "    y_proba_mlp = mlp_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    mlp_aucs.append(roc_auc_score(y_test, y_proba_mlp))\n",
    "    mlp_f1s.append(f1_score(y_test, y_pred_mlp))\n",
    "    mlp_accs.append(accuracy_score(y_test, y_pred_mlp))\n",
    "\n",
    "# --- Results ---\n",
    "print(\"MLP (5-Fold CV):\")\n",
    "print(\"ROC AUCs:\", np.round(mlp_aucs, 4))\n",
    "print(\"F1s:\", np.round(mlp_f1s, 4))\n",
    "print(\"Accuracies:\", np.round(mlp_accs, 4))\n",
    "print(f\"Mean AUC: {np.mean(mlp_aucs):.4f} | Std Dev: {np.std(mlp_aucs):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(mlp_f1s):.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(mlp_accs):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phoenix_pca]",
   "language": "python",
   "name": "conda-env-phoenix_pca-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
