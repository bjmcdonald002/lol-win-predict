{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c40119-7e67-435d-bf28-3cf1366bd8f7",
   "metadata": {},
   "source": [
    "# Binary Classification Model Comparison\n",
    "\n",
    "This notebook explores and compares multiple machine learning methods for a **binary classification problem** using ranked *League of Legends* solo/duo match data.\n",
    "\n",
    "The primary goals of this analysis are:\n",
    "- To **train and evaluate** a variety of classification models\n",
    "- To **identify top-performing models** based on validation performance\n",
    "- To **verify robustness** using both **shuffle testing** and **k-fold cross-validation**\n",
    "\n",
    "The notebook includes:\n",
    "- Data preprocessing and feature engineering\n",
    "- Implementation of various classification models (e.g., logistic regression, ensemble methods, neural networks)\n",
    "- Model evaluation and comparison\n",
    "- Post-model validation using shuffle tests and k-fold cross-validation\n",
    "\n",
    "The final result is a selection of the best-performing models, supported by rigorous validation to assess generalization performance.\n",
    "\n",
    "*Prepared by Barrett James McDonald | PhD Student, University of South Florida*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cdb34b-73c7-475f-a77d-4c0aef73b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "\n",
    "#data preprocessing & splitting\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#gradient boosting models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38db6e-307e-43ec-b9bd-e8d565ab287b",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing and Subsampling\n",
    "\n",
    "This section loads the raw match data, removes irrelevant metadata, filters for ranked solo/duo *CLASSIC* games, and prepares the cleaned numerical dataset for modeling. A random subsample of 2,500 observations is used to enable fast experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6dd224b-abf4-4300-b2be-61c15b3ac4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_csv = pd.read_csv(\"league_data.csv\", dtype={'win': str})\n",
    "\n",
    "# Drop irrelevant/metadata columns\n",
    "columns_to_drop = [\n",
    "    'game_id', 'game_version', 'participant_id', 'puuid', 'summoner_name', 'summoner_id',\n",
    "    'solo_tier', 'solo_rank', 'solo_lp', 'solo_wins', 'solo_losses',\n",
    "    'flex_tier', 'flex_rank', 'flex_lp', 'flex_wins', 'flex_losses',\n",
    "    'champion_mastery_lastPlayTime', 'champion_mastery_lastPlayTime_utc',\n",
    "    'champion_id', 'map_id', 'platform_id', 'game_type', 'team_id',\n",
    "    'game_start_utc', 'queue_id', 'game_mode'\n",
    "]\n",
    "\n",
    "# Filter for CLASSIC + ranked solo/duo games\n",
    "df_filtered = df_csv[(df_csv['game_mode'] == 'CLASSIC') & (df_csv['queue_id'] == 420)].copy()\n",
    "\n",
    "# Drop metadata columns\n",
    "df_filtered_cleaned = df_filtered.drop(columns=[col for col in columns_to_drop if col in df_filtered.columns])\n",
    "\n",
    "# Convert 'win' column to binary\n",
    "df_filtered_cleaned['win'] = (df_filtered_cleaned['win'] == 'TRUE').astype(int)\n",
    "\n",
    "# Drop non-numeric/categorical columns (and item columns)\n",
    "df_numeric_only = df_filtered_cleaned.drop(columns=df_filtered_cleaned.select_dtypes(include=['object', 'category']).columns)\n",
    "df_numeric_only = df_numeric_only.drop(columns=[col for col in df_numeric_only.columns if col.startswith(\"item\")])\n",
    "\n",
    "# Final predictor/response matrices\n",
    "X = df_numeric_only.drop(columns=['win']).fillna(df_numeric_only.mean())\n",
    "y = df_numeric_only['win']\n",
    "\n",
    "# --- Subsample preparation (2,500 observations) ---\n",
    "sample_indices = np.random.choice(X.index, size=2500, replace=False)\n",
    "X_sample = X.loc[sample_indices]\n",
    "y_sample = y.loc[sample_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e6bb0-0483-45e5-ac91-3d14767e8618",
   "metadata": {},
   "source": [
    "## 2. Modeling: Robust PCA + Logistic Regression\n",
    "\n",
    "This section uses a manually implemented **Robust PCA** to decompose the scaled predictor matrix into low-rank and sparse components. The low-rank matrix is used for further dimensionality reduction via PCA before fitting a logistic regression classifier.\n",
    "\n",
    "Performance metrics (Accuracy, F1 Score, ROC AUC) are computed on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9c5eed-f8dd-4178-b135-282702a2cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCLR Subsample Results (2,500 rows):\n",
      "Accuracy:  0.8293\n",
      "F1 Score:  0.8232\n",
      "ROC AUC:   0.8947\n"
     ]
    }
   ],
   "source": [
    "# --- Defining Robust PCA Manual Calculation of L and S ---\n",
    "def robust_pca_fast(M, max_iter=150, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Perform Robust Principal Component Analysis (RPCA) on matrix M.\n",
    "    Decomposes M into L (low-rank) and S (sparse) components using\n",
    "    the Principal Component Pursuit algorithm.\n",
    "\n",
    "    Args:\n",
    "        M (np.ndarray): Input data matrix (rows = observations, cols = features)\n",
    "        max_iter (int): Maximum number of iterations\n",
    "        tol (float): Convergence tolerance (Frobenius norm of residual)\n",
    "\n",
    "    Returns:\n",
    "        L (np.ndarray): Low-rank matrix\n",
    "        S (np.ndarray): Sparse matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Helper function: soft-thresholding operator ---\n",
    "    def shrinkage_operator(x, tau):\n",
    "        # Applies soft-thresholding elementwise\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - tau, 0.)\n",
    "\n",
    "    # --- Helper function: thresholded SVD ---\n",
    "    def svd_thresholding_operator(X, tau):\n",
    "        # Applies singular value thresholding to keep only large singular values\n",
    "        U, S, Vh = LA.svd(X, full_matrices=False)\n",
    "        S_thresh = shrinkage_operator(S, tau)\n",
    "        return U @ np.diag(S_thresh) @ Vh\n",
    "\n",
    "    # --- Initialization ---\n",
    "    S = np.zeros_like(M)              # Start with zero sparse matrix\n",
    "    Y = np.zeros_like(M)              # Lagrange multiplier (dual variable)\n",
    "    mu = np.prod(M.shape) / (4.0 * LA.norm(M, ord=1))  # Step size parameter\n",
    "    mu_inv = 1.0 / mu\n",
    "    lam = 1.0 / np.sqrt(np.max(M.shape))               # Regularization parameter\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # --- Low-rank update via SVD thresholding ---\n",
    "        L = svd_thresholding_operator(M - S + mu_inv * Y, mu_inv)\n",
    "\n",
    "        # --- Sparse matrix update via elementwise shrinkage ---\n",
    "        S = shrinkage_operator(M - L + mu_inv * Y, lam * mu_inv)\n",
    "\n",
    "        # --- Dual variable update (Lagrange multiplier) ---\n",
    "        Y = Y + mu * (M - L - S)\n",
    "\n",
    "        # --- Check convergence ---\n",
    "        error = LA.norm(M - L - S, ord='fro')\n",
    "        if error < tol:\n",
    "            break\n",
    "\n",
    "    return L, S\n",
    "\n",
    "# --- Preprocessing and robust PCA ---\n",
    "scaler_raw = StandardScaler()\n",
    "X_scaled_sample = scaler_raw.fit_transform(X_sample)\n",
    "L_sample, S_sample = robust_pca_fast(X_scaled_sample)\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(L_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- PCA and Logistic Regression ---\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_pca, y_train)\n",
    "y_pred = log_reg.predict(X_test_pca)\n",
    "y_proba = log_reg.predict_proba(X_test_pca)[:, 1]\n",
    "\n",
    "# --- Performance output ---\n",
    "print(\"PCLR Subsample Results (2,500 rows):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5fdac-77ac-470c-8de0-2cc514ee1c99",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression (No PCA): L1 vs L2 Regularization\n",
    "\n",
    "This section evaluates logistic regression models trained directly on the full set of numeric features (no dimensionality reduction).\n",
    "\n",
    "Two forms of regularization are compared:\n",
    "- **L2 (Ridge):** Penalizes the squared magnitude of coefficients. Tends to shrink coefficients uniformly but keeps all features.\n",
    "- **L1 (Lasso):** Penalizes the absolute value of coefficients. Can set some coefficients exactly to zero, thus performing feature selection.\n",
    "\n",
    "We apply both penalties to the same training/test split of a 2,500-observation subsample and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce67363-9e47-4b29-93c6-3d6e87e4cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression WITHOUT PCA\n",
      "L2 Regularization:\n",
      "Accuracy:  0.8893\n",
      "F1 Score:  0.8846\n",
      "ROC AUC:   0.9461\n",
      "\n",
      "L1 Regularization:\n",
      "Accuracy:  0.8867\n",
      "F1 Score:  0.8821\n",
      "ROC AUC:   0.9476\n"
     ]
    }
   ],
   "source": [
    "# --- Split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- L2 (Ridge) Regularization ---\n",
    "log_reg_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "log_reg_l2.fit(X_train_scaled, y_train)\n",
    "y_pred_l2 = log_reg_l2.predict(X_test_scaled)\n",
    "y_proba_l2 = log_reg_l2.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- L1 (Lasso) Regularization ---\n",
    "log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "log_reg_l1.fit(X_train_scaled, y_train)\n",
    "y_pred_l1 = log_reg_l1.predict(X_test_scaled)\n",
    "y_proba_l1 = log_reg_l1.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"Logistic Regression without PCA:\")\n",
    "\n",
    "print(\"L2 Regularization:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_l2):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_l2):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_l2):.4f}\\n\")\n",
    "\n",
    "print(\"L1 Regularization:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_l1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afab321-ca56-45e5-94a0-101cca4338d9",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Classifier\n",
    "\n",
    "This section implements a basic **Decision Tree Classifier** trained on the same scaled numeric data used in previous models.\n",
    "\n",
    "Decision Trees are intuitive and interpretable models that recursively split the feature space to classify observations. While they tend to overfit without pruning or regularization, they offer a useful baseline for tree-based methods like Random Forest and Gradient Boosted Trees.\n",
    "\n",
    "We evaluate the model on a 70/30 train-test split and report standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e04bf8a-4878-4c5a-9929-0c90bc1fbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results:\n",
      "Accuracy:  0.7667\n",
      "F1 Score:  0.7566\n",
      "ROC AUC:   0.7662\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Decision Tree ---\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test_scaled)\n",
    "y_proba_tree = tree_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_tree):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_tree):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_tree):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c0951-cded-48e2-b2dd-c08f562f158d",
   "metadata": {},
   "source": [
    "## 5. Random Forest Classifier\n",
    "\n",
    "This section applies a **Random Forest**, an ensemble learning method that builds a collection of decision trees and combines their predictions to improve generalization.\n",
    "\n",
    "Random Forests reduce overfitting by:\n",
    "- Training each tree on a random bootstrap sample of the data\n",
    "- Using a random subset of features at each split\n",
    "\n",
    "Here, we train a forest of 100 trees on a 70/30 split of the scaled data and report accuracy, F1 score, and ROC AUC. This provides a more robust baseline than a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458cc7a5-99ce-4717-a984-e52a7f8d67d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Accuracy:  0.8480\n",
      "F1 Score:  0.8417\n",
      "ROC AUC:   0.9284\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Random Forest ---\n",
    "forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "forest_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_forest = forest_clf.predict(X_test_scaled)\n",
    "y_proba_forest = forest_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_forest):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_forest):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_forest):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d73cfb-8e15-415d-88f3-91c911b06f95",
   "metadata": {},
   "source": [
    "## 6. XGBoost Classifier\n",
    "\n",
    "This section trains an **Extreme Gradient Boosting (XGBoost)** classifier, a high-performance ensemble method known for its ability to handle:\n",
    "- Imbalanced classes\n",
    "- Nonlinear feature interactions\n",
    "- Feature importance and missing data\n",
    "\n",
    "XGBoost builds trees sequentially, where each new tree tries to correct the errors of the previous one, minimizing a specified loss function—in this case, **log loss**.\n",
    "\n",
    "We train the model on scaled data with default hyperparameters and evaluate using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f634bafe-3f1b-4b1b-8ee6-4ad0d3aaeac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "Accuracy:  0.8773\n",
      "F1 Score:  0.8740\n",
      "ROC AUC:   0.9424\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train XGBoost ---\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss')\n",
    "xgb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbede2a2-a374-46f2-87ba-b4aadf21a2ca",
   "metadata": {},
   "source": [
    "## 7. LightGBM Classifier\n",
    "\n",
    "This section fits a **LightGBM (Light Gradient Boosting Machine)** model—an efficient gradient boosting framework that uses histogram-based algorithms for faster training and lower memory usage.\n",
    "\n",
    "Compared to XGBoost, LightGBM is:\n",
    "- Faster on large datasets with many features\n",
    "- Capable of handling categorical variables natively (though we use numeric-only data here)\n",
    "- Often just as accurate (or better) with less tuning\n",
    "\n",
    "We train LightGBM on a 70/30 train-test split of scaled data, evaluating its predictive performance with standard classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b53cd65d-ec1f-4297-a987-201dd18c2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Results:\n",
      "Accuracy:  0.8613\n",
      "F1 Score:  0.8575\n",
      "ROC AUC:   0.9336\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "# Convert scaled arrays back to DataFrames to retain column names and indices\n",
    "# This is useful for model types (like LightGBM) that can optionally use feature names for better interpretability,\n",
    "# and it keeps the structure consistent if we later want to analyze feature importances or visualize results.\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# --- Train LightGBM ---\n",
    "lgbm_clf = LGBMClassifier(verbose=-1)\n",
    "lgbm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_test_scaled)\n",
    "y_proba_lgbm = lgbm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"LightGBM Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_lgbm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8752382-f6c4-453b-abb7-e2c73ae3a26e",
   "metadata": {},
   "source": [
    "## 8. Support Vector Machine (RBF Kernel)\n",
    "\n",
    "This section trains a **Support Vector Machine (SVM)** with a radial basis function (RBF) kernel. SVMs aim to find the optimal hyperplane that separates classes with the **maximum margin** in a high-dimensional space.\n",
    "\n",
    "Key notes:\n",
    "- The **RBF kernel** allows the model to learn nonlinear decision boundaries by implicitly mapping features into a higher-dimensional space.\n",
    "- We set `probability=True` to enable **probability estimates**, which are required for computing the **ROC AUC** score.\n",
    "\n",
    "Although SVMs can be computationally intensive, especially on large datasets, they often perform well with clean, scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b772a541-26f2-473e-b83a-59c75e770d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Results:\n",
      "Accuracy:  0.8320\n",
      "F1 Score:  0.8274\n",
      "ROC AUC:   0.9179\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train SVM (with probability enabled for ROC AUC) ---\n",
    "svm_clf = SVC(probability=True, kernel='rbf')\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test_scaled)\n",
    "y_proba_svm = svm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Support Vector Machine Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1083e4cb-dd41-4468-a3b2-af129f2c517c",
   "metadata": {},
   "source": [
    "## 9. K-Nearest Neighbors (KNN)\n",
    "\n",
    "This section applies the **K-Nearest Neighbors (KNN)** algorithm using \\( k = 5 \\). KNN is a **non-parametric** method that classifies a sample based on the majority label among its nearest neighbors in feature space.\n",
    "\n",
    "Key characteristics:\n",
    "- Simple, interpretable, and effective with well-scaled, low-dimensional data\n",
    "- Sensitive to irrelevant features and class imbalance\n",
    "- Performance depends heavily on the choice of **k** and the distance metric\n",
    "\n",
    "Here, we standardize features and evaluate KNN on a 70/30 split of a 2,500-observation subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ce1a411-2d84-433d-9587-996780bbec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Results:\n",
      "Accuracy:  0.7813\n",
      "F1 Score:  0.7595\n",
      "ROC AUC:   0.8527\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train KNN ---\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "y_proba_knn = knn_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"K-Nearest Neighbors Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_knn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed9c0c-4edb-4c98-a314-a2366b588920",
   "metadata": {},
   "source": [
    "## 10. Naive Bayes Classifier\n",
    "\n",
    "This section implements a **Naive Bayes classifier**, specifically using the **GaussianNB** variant. Naive Bayes is a probabilistic model based on Bayes’ Theorem, assuming **feature independence** given the class label.\n",
    "\n",
    "Why it matters:\n",
    "- Surprisingly effective in high-dimensional settings\n",
    "- Fast to train and easy to interpret\n",
    "- Works best when features are roughly independent (which is rare, but doesn’t always hurt performance)\n",
    "\n",
    "We fit the model on scaled data and evaluate using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2213e8db-5077-43d4-9025-c916c98609e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Accuracy:  0.6880\n",
      "F1 Score:  0.6422\n",
      "ROC AUC:   0.7606\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Naive Bayes ---\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_test_scaled)\n",
    "y_proba_nb = nb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_nb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556a17a-8fc4-41ab-aeff-ccf943e6c79d",
   "metadata": {},
   "source": [
    "## 11. Neural Network: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "This final model is a **Neural Network**, specifically a **Multi-Layer Perceptron (MLP)**. MLPs are **feedforward neural networks** that learn complex, nonlinear relationships through layers of interconnected neurons.\n",
    "\n",
    "Model architecture:\n",
    "- One hidden layer with 100 neurons\n",
    "- Uses ReLU activation (default)\n",
    "- Optimized with the Adam solver\n",
    "- Trained for up to 500 iterations (or until convergence)\n",
    "\n",
    "While MLPs often require more tuning and training time, they can outperform traditional models when the data contains deep, abstract patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9727e336-d259-4be2-9834-7f462f910cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network (MLP) Results:\n",
      "Accuracy:  0.8760\n",
      "F1 Score:  0.8724\n",
      "ROC AUC:   0.9424\n"
     ]
    }
   ],
   "source": [
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train Neural Network (MLP) ---\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)\n",
    "mlp_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_mlp = mlp_clf.predict(X_test_scaled)\n",
    "y_proba_mlp = mlp_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Neural Network (MLP) Results:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_mlp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1ac1b-03a4-444a-bb94-112e56199ea5",
   "metadata": {},
   "source": [
    "## 12. Tuned Neural Network (MLP) on Full Dataset\n",
    "\n",
    "In this final model, we revisit the **Multi-Layer Perceptron (MLP)** and apply **hyperparameter tuning** for improved performance. Key changes include:\n",
    "\n",
    "- **Architecture:** Two hidden layers with 128 and 64 neurons\n",
    "- **Activation:** ReLU (Rectified Linear Unit)\n",
    "- **Optimizer:** Adam (adaptive learning rate)\n",
    "- **Training:** 1,000 epochs (or until convergence)\n",
    "- **Dataset:** Full cleaned dataset (not a subsample)\n",
    "\n",
    "These changes aim to increase the model's capacity to learn complex, nonlinear relationships across the full feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8beb917c-81ac-464d-9741-2528c9d873c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Neural Network Results (Full Dataset):\n",
      "Accuracy:  0.8778\n",
      "F1 Score:  0.8790\n",
      "ROC AUC:   0.9461\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, stratify=y_full)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Tuned MLP Neural Network ---\n",
    "mlp_tuned = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=1000)\n",
    "mlp_tuned.fit(X_train_scaled, y_train)\n",
    "y_pred_mlp = mlp_tuned.predict(X_test_scaled)\n",
    "y_proba_mlp = mlp_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"Tuned Neural Network Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_mlp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd283a-a7af-4572-93cd-ea82634ed3de",
   "metadata": {},
   "source": [
    "## 13. LightGBM on Full Dataset\n",
    "\n",
    "This model revisits **LightGBM**, this time training on the entire preprocessed dataset rather than a subsample.\n",
    "\n",
    "By scaling and fitting LightGBM on the full data, we aim to:\n",
    "- Leverage more information for training\n",
    "- Capture rarer patterns that may not be present in smaller subsamples\n",
    "- Potentially improve performance and stability\n",
    "\n",
    "We retain feature names by converting the scaled arrays back into DataFrames, which can help with future interpretability and feature importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0056dde2-db4a-439d-838a-9518524f6a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Results (Full Dataset):\n",
      "Accuracy:  0.8640\n",
      "F1 Score:  0.8599\n",
      "ROC AUC:   0.9427\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, stratify=y_sample)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# --- Train LightGBM ---\n",
    "lgbm_clf = LGBMClassifier(verbose=-1)\n",
    "lgbm_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_test_scaled)\n",
    "y_proba_lgbm = lgbm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"LightGBM Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_lgbm):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_lgbm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e425f-cd42-4ecf-9f66-8b5d5990e40b",
   "metadata": {},
   "source": [
    "## 14. XGBoost on Full Dataset\n",
    "\n",
    "This section returns to **XGBoost**, but this time we train on the entire preprocessed dataset instead of a subsample.\n",
    "\n",
    "Key details:\n",
    "- Full dataset provides a richer training signal, potentially capturing subtler relationships\n",
    "- XGBoost is configured to minimize **log loss**, which is appropriate for binary classification with probabilistic outputs\n",
    "- Data is standardized before training, though XGBoost can handle unscaled input—scaling helps ensure fair comparison across models\n",
    "\n",
    "We evaluate model performance using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ee57ff7-f87e-4d2d-aa68-b123abd2bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results (Full Dataset):\n",
      "Accuracy:  0.8972\n",
      "F1 Score:  0.8964\n",
      "ROC AUC:   0.9648\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, stratify=y_full)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Train XGBoost ---\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss')\n",
    "xgb_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"XGBoost Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0725d4-84f8-4580-9d15-39f8c8ff24b1",
   "metadata": {},
   "source": [
    "## 15. L1-Regularized Logistic Regression on Full Dataset\n",
    "\n",
    "Here we train a **logistic regression model with L1 (Lasso) regularization** on the full dataset.\n",
    "\n",
    "L1 regularization:\n",
    "- Encourages sparsity in the model coefficients (i.e., some become exactly zero)\n",
    "- Effectively performs **feature selection** by shrinking less informative features\n",
    "- Helps prevent overfitting when many features are present\n",
    "\n",
    "We scale the full dataset before training, and evaluate using accuracy, F1 score, and ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95ff6f26-5ecf-48b3-b272-38a925117434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression Results (Full Dataset):\n",
      "Accuracy:  0.8848\n",
      "F1 Score:  0.8826\n",
      "ROC AUC:   0.9482\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Train/test split and scale ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.3, stratify=y_full)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- L1 (Lasso) Logistic Regression ---\n",
    "log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "log_reg_l1.fit(X_train_scaled, y_train)\n",
    "y_pred_l1 = log_reg_l1.predict(X_test_scaled)\n",
    "y_proba_l1 = log_reg_l1.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Evaluate performance ---\n",
    "print(\"L1 Logistic Regression Results (Full Dataset):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_l1):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_proba_l1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e60f0f-7a08-4d84-a6d1-2438fe174ae9",
   "metadata": {},
   "source": [
    "## 16. Shuffle Test for Model Validation (LightGBM & XGBoost)\n",
    "\n",
    "To validate the performance of the top-performing models (**LightGBM** and **XGBoost**), we perform a **shuffle test**:\n",
    "\n",
    "- The target labels (`y`) are randomly permuted, breaking any true association between the features (`X`) and the outcome.\n",
    "- We then train and evaluate the models on this **mismatched X/y** pairing.\n",
    "- If model performance drops significantly (as it should), we gain confidence that the original performance was not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0337368-4167-48ee-84ca-f519046ed6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM (Shuffled Labels):\n",
      "Accuracy:  0.5109\n",
      "F1 Score:  0.5154\n",
      "ROC AUC:   0.5124\n",
      "\n",
      "XGBoost (Shuffled Labels):\n",
      "Accuracy:  0.4934\n",
      "F1 Score:  0.4944\n",
      "ROC AUC:   0.4923\n"
     ]
    }
   ],
   "source": [
    "# --- Shuffle labels independently ---\n",
    "y_shuffled = y_full.sample(frac=1).reset_index(drop=True)\n",
    "X_shuffled = X_full.reset_index(drop=True)  # Align index with shuffled y\n",
    "\n",
    "# --- Train/test split and scale (on mismatched X/y) ---\n",
    "X_train, X_test, y_train_shuffled, y_test_shuffled = train_test_split(\n",
    "    X_shuffled, y_shuffled, test_size=0.3, stratify=y_shuffled)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# --- LightGBM on shuffled labels ---\n",
    "lgbm_clf = LGBMClassifier(verbose=-1)\n",
    "lgbm_clf.fit(X_train_scaled, y_train_shuffled)\n",
    "y_pred_lgbm = lgbm_clf.predict(X_test_scaled)\n",
    "y_proba_lgbm = lgbm_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- XGBoost on shuffled labels ---\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss')\n",
    "xgb_clf.fit(X_train_scaled, y_train_shuffled)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# --- Results ---\n",
    "print(\"LightGBM (Shuffled Labels):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test_shuffled, y_pred_lgbm):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test_shuffled, y_pred_lgbm):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test_shuffled, y_proba_lgbm):.4f}\\n\")\n",
    "\n",
    "print(\"XGBoost (Shuffled Labels):\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test_shuffled, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test_shuffled, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test_shuffled, y_proba_xgb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a320b2d1-a831-4593-84c3-20f9e7e32d20",
   "metadata": {},
   "source": [
    "## 17. K-Fold Cross-Validation (LightGBM & XGBoost)\n",
    "\n",
    "As a final step in validating model performance, we apply **5-fold stratified cross-validation** to the top two models: **LightGBM** and **XGBoost**.\n",
    "\n",
    "This method ensures that:\n",
    "- Every observation is used in both training and testing\n",
    "- Class proportions are preserved in each fold (stratified)\n",
    "- We obtain a **distribution of ROC AUC scores** across folds, allowing us to assess stability and generalization\n",
    "\n",
    "Results are reported as individual fold AUCs, along with their mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef87a051-6b98-4c97-b35a-d673c03af150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM K-Fold ROC AUCs: [0.9652 0.9671 0.9647 0.9653 0.966 ]\n",
      "Mean AUC: 0.9657 | Std Dev: 0.0008\n",
      "\n",
      "XGBoost K-Fold ROC AUCs: [0.9664 0.9666 0.9637 0.9661 0.9647]\n",
      "Mean AUC: 0.9655 | Std Dev: 0.0011\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Scale full data ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index) #will need to be a dataframe with named columns for the loop below\n",
    "\n",
    "# --- 5-Fold Stratified CV ---\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "lgbm_aucs = []\n",
    "xgb_aucs = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled_df, y_full):\n",
    "    X_train, X_test = X_scaled_df.iloc[train_index], X_scaled_df.iloc[test_index]\n",
    "    y_train, y_test = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm_model = LGBMClassifier(verbose=-1)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    lgbm_proba = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "    lgbm_auc = roc_auc_score(y_test, lgbm_proba)\n",
    "    lgbm_aucs.append(lgbm_auc)\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = XGBClassifier(eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
    "    xgb_aucs.append(xgb_auc)\n",
    "\n",
    "# --- Results ---\n",
    "print(\"LightGBM K-Fold ROC AUCs:\", np.round(lgbm_aucs, 4))\n",
    "print(f\"Mean AUC: {np.mean(lgbm_aucs):.4f} | Std Dev: {np.std(lgbm_aucs):.4f}\\n\")\n",
    "\n",
    "print(\"XGBoost K-Fold ROC AUCs:\", np.round(xgb_aucs, 4))\n",
    "print(f\"Mean AUC: {np.mean(xgb_aucs):.4f} | Std Dev: {np.std(xgb_aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0630b3b-5ebb-4542-a902-c3fdde5212be",
   "metadata": {},
   "source": [
    "## 18. L1-Regularized Logistic Regression: Shuffle Test & Cross-Validation\n",
    "\n",
    "To validate the performance of the L1-regularized logistic regression model, we apply two key methods:\n",
    "\n",
    "### 1. Shuffle Test\n",
    "- The response variable (`y`) is randomly shuffled to break any feature-target relationships.\n",
    "- The model is then retrained and evaluated.\n",
    "- A **low ROC AUC** score indicates that the model’s original performance was based on **real structure** in the data.\n",
    "\n",
    "### 2. 5-Fold Stratified Cross-Validation\n",
    "- We use stratified folds to preserve class balance.\n",
    "- Performance is measured using **ROC AUC** across all folds.\n",
    "- We report both the fold-wise scores and their **mean and standard deviation**.\n",
    "\n",
    "These validation steps provide confidence that the logistic model generalizes well and is not simply overfitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97eab939-299f-4c65-86ec-221e7a18bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Logistic Regression – Shuffle Test\n",
      "ROC AUC (Shuffled): 0.5040\n",
      "\n",
      "L1 Logistic Regression – 5-Fold Cross-Validation\n",
      "AUCs: [0.9513 0.9426 0.9478 0.9533 0.943 ]\n",
      "Mean AUC: 0.9476 | Std Dev: 0.0043\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_full = X.copy()\n",
    "y_full = y.copy()\n",
    "\n",
    "# --- Scale full data ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "# ------------------------------\n",
    "# Shuffle Test (L1 Logistic)\n",
    "# ------------------------------\n",
    "_, y_shuffled = shuffle(X_scaled, y_full)\n",
    "\n",
    "X_train, X_test, y_train_shuff, y_test_shuff = train_test_split(X_scaled, y_shuffled, test_size=0.3, stratify=y_shuffled)\n",
    "\n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "logreg_l1.fit(X_train, y_train_shuff)\n",
    "y_proba_shuff = logreg_l1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"L1 Logistic Regression – Shuffle Test\")\n",
    "print(f\"ROC AUC (Shuffled): {roc_auc_score(y_test_shuff, y_proba_shuff):.4f}\")\n",
    "print()\n",
    "\n",
    "# ------------------------------\n",
    "# 5-Fold Cross-Validation\n",
    "# ------------------------------\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_scaled, y_full):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "    \n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "print(\"L1 Logistic Regression – 5-Fold Cross-Validation\")\n",
    "print(\"AUCs:\", np.round(auc_scores, 4))\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.4f} | Std Dev: {np.std(auc_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3700dac-9237-4341-92f1-21eabaede48e",
   "metadata": {},
   "source": [
    "## 19. Neural Network Validation: Shuffle Test & Cross-Validation\n",
    "\n",
    "To ensure the reliability of the tuned **MLP neural network**, we validate its performance using both:\n",
    "\n",
    "### 1. Shuffle Test\n",
    "- The target labels are randomly permuted to destroy true patterns.\n",
    "- A strong model should perform poorly in this setting.\n",
    "- A ROC AUC near 0.5 confirms the model is not just fitting noise.\n",
    "\n",
    "### 2. 5-Fold Stratified Cross-Validation\n",
    "- The dataset is split into 5 stratified folds to preserve class distribution.\n",
    "- The model is trained and evaluated across all folds.\n",
    "- Mean and standard deviation of ROC AUC scores are reported to assess generalizability.\n",
    "\n",
    "This double validation confirms whether the neural network is truly learning structure from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c21b815-e3ff-4a86-92eb-493661f5ff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP – Shuffle Test\n",
      "ROC AUC (Shuffled): 0.4990\n",
      "\n",
      "MLP – 5-Fold Cross-Validation\n",
      "AUCs: [0.8819 0.8776 0.8375 0.7912 0.8698]\n",
      "Mean AUC: 0.8516 | Std Dev: 0.0340\n"
     ]
    }
   ],
   "source": [
    "# --- Full dataset (already preprocessed into X and y) ---\n",
    "X_nn = X.copy()\n",
    "y_nn = y.copy()\n",
    "\n",
    "# =========================\n",
    "# Shuffle Test\n",
    "# =========================\n",
    "_, y_shuffled = shuffle(X_nn, y_nn)\n",
    "\n",
    "X_train, X_test, y_train_shuff, y_test_shuff = train_test_split(\n",
    "    X_nn, y_shuffled, test_size=0.3, stratify=y_shuffled\n",
    ")\n",
    "\n",
    "mlp_shuff = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', max_iter=1000)\n",
    "mlp_shuff.fit(X_train, y_train_shuff)\n",
    "y_proba_shuff = mlp_shuff.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"MLP – Shuffle Test\")\n",
    "print(f\"ROC AUC (Shuffled): {roc_auc_score(y_test_shuff, y_proba_shuff):.4f}\\n\")\n",
    "\n",
    "# =========================\n",
    "# 5-Fold Cross-Validation\n",
    "# =========================\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "auc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_nn, y_nn):\n",
    "    X_train, X_test = X_nn.iloc[train_index], X_nn.iloc[test_index]\n",
    "    y_train, y_test = y_nn.iloc[train_index], y_nn.iloc[test_index]\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "print(\"MLP – 5-Fold Cross-Validation\")\n",
    "print(\"AUCs:\", np.round(auc_scores, 4))\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.4f} | Std Dev: {np.std(auc_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phoenix_pca]",
   "language": "python",
   "name": "conda-env-phoenix_pca-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
